# 운전점수 2.0 테스트 및 환경 구축, 개발

새로운 운전점수 로직 및 개발을 했는데, 이 로직이 맞는지 확인해야 하잖아. 어떻게 했냐면 기존 유저의 10%만 실제 데이터를 추출해서, 데이터베이스에 올리고, 데이터 엔지니어에게 변경된 로직대로 시물레이션을 요청해, 
그럼 그거 가지고, 우리가 원하는 결과가 나오는지 판단하고 factor를 계속 변경해서, 원하는 결과가 나올 때 까지 해봐 
그런 다음, 그 로직을 애플리케이션에 적용을 해야해. 
가장 어려운 점은 유저가 모르게, 1.0 점수에서 2.0 점수로 마이그레이션이 점진적으로 되어야 한다는거야
이게 가장 어려웠고, 어떻게 했는지는 아직 진행중이라 변하기 어려 울듯

그리고 실제로 변경된 로직을 적용하기전에, 운영 데이터 일부를 테스트 환경으로 보내면서 특정 기간 동안 테스트 할 수 있는거지 MSK에서 consumer 하나 만들어서 그리로 보내면 되는거지



# 운전점수 라이브러리 개발

프로젝트가 msa로 되어 있어서 다수의 프로젝트가 똑같은 운전점수 로직 계산을 갖고 있는거야
그래서 그 로직을 라이브러리화 해버림 com.tmapmobility.score:driving-score 이런 프로젝트 import 하면 
모두 다 동일한 운전점수 로직을 사용할 수 있지. 운전점수 2.0에서 빛을 좀 많이 봤어 왜냐하면 라이브러리만 바꾸고, 이걸 쓰는 쪽 코드만 좀 변경하면 금방 되니까 많이 좋았지



# 운전습관 위반요소 CS 용이하게 만듬

운전습관은 Flink 프로젝트인데 사용자가 운전점수나 위반요소에 대해 VOC를 남긴단 말이야
그럴 때 매번 CS 담당자가 superset에 질의해서 볼수 밖에 없었어
그러다 보니 superset 질의하는것도 매번 돈이고, airflow에서 cs 담당자가 보기 좋게 한번 더 가공해주는 dag이 있었는데, 이 dag의 요금도 만만치 않았지. 그래서 이런 문제를 해결하려 했어

raw 데이터 -> s3 적재 -> airflow로 가공해서 athena로 적재 -> superset에서 질의 하는것을 스퀘어어드민에서 바로 볼수 있게 했어

하지만 Kafka 메세지 용량이 커지는 트레이드오프가 있었어, 운전습관에서 운전점수 서비스로 주행을 전달해 줄 때는 위반요소의 디테일 까지는 주지 않았지만 이 변경 때문에 디테일까지 필요해서 Kafka 메세지 크기가 늘어난거야. 
이로 인해 운전습관 장애가 한번 있었어. 뭐냐면 카프카 기본 설정이 메세지 크기가 1MB더라고, 그래서 이것때문에 Flink가 Down 되는일이 있었지, (Flink는 Exception이 일어나면 Down되고, 다시 재복구를 하니까)



# 운전습관(Flink) 로컬환경 구성 및 테스트

VOC가 들어 왔을 때, 왜 이렇게 위반됐고, 원인이 뭐고 이런걸 다 밝혀야 하잖아. 
근데 주행이란건 엄청 많은 raw 데이터의 집합이다보니까 사람의 눈으로 하나씩 보기가 힘들어. 
그럼 가장 간단한거는 로컬환경에서 사용자가 주행했던 GPS를 그대로 재적재해서 consume 해보는거야
내가 인수인계 받았을 땐, 이런 문서나 방법론을 아무도 안쓰고 있어서 내가 최초로 IDE로 로컬환경으로 띄워보고, GPS를 consuem할 수 있는 테스트 환경을 만들었어

그래서 어떤 이슈가 있더라고, 리플레이가 가능하니까, 디버깅이 엄청 수월해졌지
VOC 한건당 20분 걸리는걸 이제 2분이면 다 해







# 운전습관 (Flink) 로직 안정화

GPS와 MapLink 매칭 로직 버그로 인한 주행 이력 미생성이 되었어. 
일단은 GPS와 MapLink간의 최대 맵매칭 거리가 몇인지 그리고, 로직을 이렇게 변경했을 때 영향도는 없을지  기준을 정해야 하기 때문에, 먼저 메트릭을 생성했음 



- 터널 진입 시 mapLinkList.time = 0 처리 문제

- - **현상**

  - 1. 터널 내부에서는 GPS가 맵 매칭되지 않아 mapLinkList.time = 0 으로 전달
    2. 그 결과 직전(터널 진입 전) 맵링크에 매칭되어, 터널 내부임에도 과속 등 위반 요소가 검출됨

  - **원인**

  - 1. 수선의 발(map-matching) 로직에 “최대 허용 매칭 거리” 제한이 없어,
        터널 구간의 GPS 포인트를 이전 맵링크로 그대로 연결

  - **개선 방안**

  - 1. **매칭 거리 제한 도입**

    2. - 수선의 발 로직에 MAX_MATCH_DISTANCE 파라미터 추가 (예: 50–100m)
       - 임계 거리 초과 시 해당 GPS 포인트는 매칭 대상에서 제외

    3. **터널 구간 별도 처리**

    4. - 터널 진입 시점에 isTunnel=true 플래그 설정
       - 터널 내부의 매칭 실패 포인트는 “위반검출 제외” 로직 적용

- 초반 GPS 포인트가 매우 먼 맵링크에 매칭되는 버그

- - **현상**

  - 1. 주행 시작 직후 GPS 포인트가 실제와 멀리 떨어진(예: 39 km 이상) 맵링크에 매칭
    2. 이후 이전 맵링크들은 배제되어, 주행 이력이 생성되지 않거나 일부 구간만 생성

  - **원인**

  - 1. 초기 매칭 단계에서 “현재 기준 맵링크” 가 제대로 설정되지 않아,
        가장 가까운 후보가 아닌 먼 맵링크에 매칭
    2. 이후 로직상 “한 번 정해진 기준 맵링크” 이외의 후보는 모두 무시

  - **개선 방안**

  - 1. **초기 매칭 거리 제한**

    2. - 시작 포인트도 MAX_MATCH_DISTANCE 적용
       - 임계 거리 내 후보가 없으면 “매칭 대기” 상태로 두었다가 다음 포인트 재시도

    3. **매칭 후보 재검토 로직 추가**

    4. - 기준 맵링크가 확실치 않을 경우, 직전 N개 포인트와의 평균 거리·방향을 고려해 보정
       - 단일 포인트에 의존하지 않도록 회귀 테스트 강화





# 에이닷 주행 이력 제공

이건 SKT에서 에이닷 AI 기능을 많이 활용하기 위한 프로젝트야. 
에이닷을 동의한 사용자에게 데일리로 주행이력을 전달한거지 

이 때 airflow로 에이닷을 동의한 사용자의 주행이력을 특정 s3 적재하는 일을 했어

이력서에 넣을까 고민이긴해, 나는 백엔드만 작업했고 airflow 경험은 처음인거라, 어필이 될 수 있겠지만, airflow를 그렇게 잘 아는편은 아니라서



# 운전습관 (Flink) 장애시 Offset Rewind 할 때, 정합성을 잃어버리는 이슈 (40%정도 주행이 이상해짐) <- 가장 중요

내 사상은 모든 애플리케이션은 장애가 필연적으로 발생하고, 이를 복구할 수 있는 장애 복구성을 갖춰야 한다고 생각해 
운전습관은 장애가 발생하면, 특정 시점 (예를 들어 12시간전)으로 Kafka Offset을 되돌려서 그때의 주행부터 전부 재처리를해 
그럼 중복이 발생하지 않냐 물어볼수 있지만, consumer 쪽에서 userKey 와 주행ID로 멱등성을 검증하고 있기 때문에 중복이 발생할 일이 없어. 

하지만 실제로 재처리를 하니까 40%의 주행이 새로 생기는 현상이 발생한거야.
이 이슈의 원인을 찾는데 까지 엄청 힘들었다. 

로컬에서는 재현이 안되고 무조건 데이터가 많은 환경에서만 재현이 되는거야
그래서 일단 나는 새로운 환경을 만들었어, (stg-test 라고 부를 께) 그리고 데이터베이스는 DyanmoDB를 사용했는데, 새로운 환경에선 RDB를 활용했어. 그 이유는 데이터 정합성이 잘 맞는지 쿼리로 검증할려 했거든, 근데 DynamoDB는 그게 안됐어. 
그리고, DynamoDB는 인덱스를 한번 걸면 변경하지 못하는 특성 때문에 테스트가 힘들었어(GSI등, 틀리면 정정바람)
그래서 RDB를 선택했어 

그럼 이게 올바른 데이터다! 라는 기준이 있어야 할거 아니야? 그걸 baseLine으로 지칭할께
운전습관은 consumer가 msk라서 stg-test를 운영환경의 msk로 새로운 consumer를 붙어서 실시간으로 데이터를 쌓았어. 즉 이게 baseLine이야. 

그다음 RDB 새로은 스키마를 하나 더 만들어서 rewindTest라는 스키마를 만들어서, kafka offset rewind해서 데이터를 쌓았지. 

그렇게 해서 쿼리로 비교해서, 문제가 있구나 알게 된거야 

하지만 msk 리텐션이 3일이라서 baseline 데이터는 3일밖에 존재하지 않아. 그말은 즉슨 예상 원인을 테스트하려면 3일밖에 없다는거야.

이게 테스트가 너무 불편해서 msk 신규 클러스터를 만들어서 리텐션 무제한으로 했어. 이렇게 한 이유는 msk는 한번 disk를 증설하면 내릴수 없는 단점이 있더라고, 
운전습관은 하루에 2TB씩 MSK를 쓰고 있어서 운영환경 MSK를 그대로 쓰긴 어려워서 신규 환경을 만들었어 

결론은 공식문서 보면서 해결했어 

kafka 파티션은 90개인데 flink consumer는 50개다 보니까. 1개의 consumer가 2개의 kafka 파티션을 맡았는데, Watermark가 2개의 파티션중에 가장 빠른 파티션을 기준으로 watermakr를 생성하고 있었기 때문에, 다른 파티션 1개는 late data로 drop이 되버린거지 

결국 해결했지만, 테스트 과정과 원인 해결 과정이 너무 힘들었음. 

